{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hindi Letter Recognition system"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTING REQUIRED MODULES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import glob\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import decomposition\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONVERTING IMAGE DATA TO CSV OF ARRAY OF PIXELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = [\"label\"]\n",
    "for i in range(0, 1024):\n",
    "    header.append(f\"pxl_{i}\")\n",
    "\n",
    "with open(\"pixel_data.csv\", \"w\", newline = '') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "\n",
    "IMG_DIR = 'MainData/Data'\n",
    "for label in os.listdir(IMG_DIR):\n",
    "    dirlist = glob.glob(f\"{IMG_DIR}/{label}/*.png\")\n",
    "\n",
    "    for img_path in dirlist:\n",
    "        img = cv2.imread(img_path)\n",
    "        img_grey = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        img_grey = cv2.GaussianBlur(img_grey, (15,15), 0)\n",
    "        roi = cv2.resize(img_grey, (32, 32), interpolation = cv2.INTER_AREA)\n",
    "        data = []\n",
    "        data.append(label)\n",
    "\n",
    "        rows, cols = roi.shape\n",
    "\n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                k = roi[i, j]\n",
    "                data.append(k)\n",
    "\n",
    "        with open(\"pixel_data.csv\", \"a\", newline = '') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(data)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## READING THE pixel_data.csv FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('pixel_data.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TAKING 500 IMAGES FOR EACH CHARACTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = data.groupby(\"label\")\n",
    "data = grouped_data.head(500)\n",
    "data = shuffle(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEPARATING FEATURES AND LABELS FROM PIXEL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data.drop([\"label\"], axis = 1)\n",
    "labels = data[\"label\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPLITTING THE DATA INTO TEST AND TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(features, labels, test_size = 0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENCODING TEST AND TRAIN LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelEncoder()\n",
    "train_y_encoded = lb.fit_transform(train_y)\n",
    "test_y_encoded = lb.transform(test_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping the encoded values to their actual names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {\n",
    "    0: \"क्ष\",\n",
    "    1: \"त्र\",\n",
    "    2: \"ज्ञ\",\n",
    "    3: \"ग\",\n",
    "    4: \"घ\",\n",
    "    5: \"ड\",\n",
    "    6: \"च\",\n",
    "    7: \"छ\",\n",
    "    8: \"ज\",\n",
    "    9: \"झ\"\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCALING TRAIN AND TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train_x_scaled = scaler.fit_transform(train_x)\n",
    "test_x_scaled = scaler.transform(test_x)\n",
    "train_x_scaled_df = pd.DataFrame(train_x_scaled, columns=features.columns)\n",
    "test_x_scaled_df = pd.DataFrame(test_x_scaled, columns=features.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USING PCA(PRINCIPAL COMPONENT ANALYSIS) FOR DIMENSIONALITY REDUCTION ON TRAIN AND TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_model = decomposition.PCA()\n",
    "n_comp = 85\n",
    "pca_model.n_components = n_comp\n",
    "pca_data_train = pca_model.fit_transform(train_x_scaled_df)\n",
    "pca_data_test = pca_model.transform(test_x_scaled_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATING THE REQUIRED PANDAS DATAFRAME FROM THE NUMPY-ND ARRAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "for i in range(n_comp):\n",
    "    lst.append(f\"f{i+1}\")\n",
    "pca_df_train = pd.DataFrame(data = pca_data_train, columns = tuple(lst))\n",
    "pca_df_test = pd.DataFrame(data = pca_data_test, columns = tuple(lst))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSNE AFTER PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "dimensions = 3\n",
    "perplexity = 5\n",
    "n_iter = 5000\n",
    "\n",
    "tsne_model = TSNE(n_components = dimensions, perplexity = perplexity, random_state = 42, n_iter = n_iter, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_data_train = tsne_model.fit_transform(pca_df_train)\n",
    "tsne_data_test = tsne_model.fit_transform(pca_df_test)\n",
    "tsne_df_train = pd.DataFrame(data = tsne_data_train, columns = (\"f1\", \"f2\",\"f3\"))\n",
    "tsne_df_test = pd.DataFrame(data = tsne_data_test, columns = (\"f1\", \"f2\",\"f3\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHECKING THE ACCURACY OF THE NN TRAINED MODEL ON TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eval = classifier_nn.evaluate(tsne_df_train, train_y_encoded, verbose=0)\n",
    "test_eval = classifier_nn.evaluate(tsne_df_test, test_y_encoded, verbose=0)\n",
    "h = [\"loss\", \"accuracy\"]\n",
    "l = [train_eval, test_eval]\n",
    "eval_df = pd.DataFrame(l, columns = h)\n",
    "eval_df.insert(loc = 0, column = \"Data\", value = [\"Train\", \"Test\"])\n",
    "acc_pca_tSNE_nn = round(eval_df['accuracy'][1], 3)\n",
    "eval_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model using pca"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SVC(kernel = \"linear\", random_state = 6)\n",
    "classifier.fit(pca_df_train, train_y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the accuracy of SVC model\n",
    "prediction = classifier.predict(pca_df_test)\n",
    "accuracy = metrics.accuracy_score(prediction, test_y_encoded)\n",
    "acc_pca_svc = accuracy\n",
    "print(metrics.classification_report(prediction, test_y_encoded, target_names = classes.values()))\n",
    "print(f\"PCA-SVM accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Training model with neural network\n",
    "classifier_nn = tf.keras.models.Sequential()\n",
    "classifier_nn.add(tf.keras.layers.Dense(128, activation = 'relu'))\n",
    "classifier_nn.add(tf.keras.layers.Dense(10, activation = 'softmax'))\n",
    "classifier_nn.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "history = classifier_nn.fit(pca_df_train, train_y_encoded, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv1D, MaxPooling2D, GlobalMaxPool1D, Embedding, Activation, Flatten,Input\n",
    "from keras.layers import Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_model.predict(pca_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eval = classifier_nn.evaluate(pca_df_train, train_y_encoded, verbose=0)\n",
    "test_eval = classifier_nn.evaluate(pca_df_test, test_y_encoded, verbose=0)\n",
    "h = [\"loss\", \"accuracy\"]\n",
    "l = [train_eval, test_eval]\n",
    "eval_df = pd.DataFrame(l, columns = h)\n",
    "eval_df.insert(loc = 0, column = \"Data\", value = [\"Train\", \"Test\"])\n",
    "acc_pca_nn = round(eval_df['accuracy'][1], 3)\n",
    "eval_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
